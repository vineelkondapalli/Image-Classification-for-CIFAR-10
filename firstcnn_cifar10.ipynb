{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOuzhxGFei3mTz+b45KnT/o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineelkondapalli/Image-Classification-for-CIFAR-10/blob/main/firstcnn_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We are building a CNN for CIFAR-10.**\n",
        "\n",
        "Defining Transform\n",
        "\n",
        "*   We turn the image into a tensor and normalize the values to allow faster training of the model.\n",
        "\n",
        "*  It essentially turns the loss functiono space into a more even surface, making gradient descent faster.\n",
        "\n",
        "Setting up training data\n",
        "\n",
        "\n",
        "*   set batch size kinda high bc we have colab pro gpus\n",
        "*   then we use dataset and dataloader to load in the data\n",
        "\n",
        "\n",
        "*   Setting num_workers to 8 because we have access to more compute. It basically allows more proccesses to run in parallel"
      ],
      "metadata": {
        "id": "W6kARIK8C2-8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "956CIZoJ8BDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5961eca-9e10-42b5-ec57-080ecb97239d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu128\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Define the transformations for the data\n",
        "# We will convert the images to Tensors and normalize them\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),#data augmentation\n",
        "    transforms.RandomHorizontalFlip(),#makes the data more well rounded my introducing rotations\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #makes the loss function surface more even for training\n",
        "    transforms.RandomErasing()# more data aug. introduces erasing parts of the\n",
        "])\n",
        "\n",
        "# Define transformations for the test set (no augmentation)\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# 2. Set the batch size\n",
        "batch_size = 256\n",
        "\n",
        "# Reload the datasets with the new transformations\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, # Using a larger batch size\n",
        "                                          shuffle=True, num_workers=8)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n",
        "                                         shuffle=False, num_workers=8)\n",
        "\n",
        "# 5. Define the classes\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Residual Blocks to replace conv layers, in order to avoid vanishing gradients(NEW MODEL)"
      ],
      "metadata": {
        "id": "cpjgVF1Xk2vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut path to handle dimension changes\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x) # Add the shortcut connection\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.in_channels = 32\n",
        "\n",
        "        # Initial \"stem\" layer\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Stacking the residual blocks\n",
        "        self.layer1 = ResidualBlock(32, 64, stride=2)\n",
        "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
        "\n",
        "        # Classifier head\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "s3pPXUuik95D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the model and forward pass(OLD MODEL)**\n",
        "\n",
        "\n",
        "1.   using the init function to define the params for each of our layers, input and output size\n",
        "2.   then using the defined layers in forward to define what a forward pass looks like on a singular batch of data.\n",
        "\n"
      ],
      "metadata": {
        "id": "RoMvsloGD_Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         # Layer Block 1\n",
        "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2) # Added padding\n",
        "#         self.bn1 = nn.BatchNorm2d(32) # Added Batch Norm\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "#         # Layer Block 2\n",
        "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2) # Added padding\n",
        "#         self.bn2 = nn.BatchNorm2d(64) # Added Batch Norm\n",
        "\n",
        "#         # Layer Block 3 (New)\n",
        "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=5, padding=2) # New conv layer\n",
        "#         self.bn3 = nn.BatchNorm2d(128) # New Batch Norm\n",
        "\n",
        "#         # Fully connected layers\n",
        "#         # Input size is now 128 * 4 * 4 = 2048\n",
        "#         self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "#         self.fc2 = nn.Linear(256, 128)\n",
        "#         self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "#         #defining the dropout layer\n",
        "#         self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Block 1\n",
        "#         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "#         # Block 2\n",
        "#         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "#         # Block 3\n",
        "#         x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "#         # Flatten and classify\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.dropout(F.relu(self.fc1(x)))\n",
        "#         x = self.dropout(F.relu(self.fc2(x)))\n",
        "#         x = self.fc3(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "7BCJlj6B8-q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define the Loss function and optimizer\n",
        "\n",
        "We choose Cross Entropy loss to keep a smooth convex loss surface to make training faster. CLE runs a softmax over the logits and then computes the -log of the correct probabiility."
      ],
      "metadata": {
        "id": "E1gnSRX5NT_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "net = Net()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "net.to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001) #old optimizer and scheduler\n",
        "# scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
        "\n",
        "print(\"Loss function and optimizer are defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKJkW8a4NTUL",
        "outputId": "6e2b588d-911e-4031-91d4-5461362f781c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function and optimizer are defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to train!"
      ],
      "metadata": {
        "id": "7ihfbSZjOlSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop for 25 epochs\n",
        "for epoch in range(50):\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    net.train() # Use 'model' if you're using the ResNet, or 'net' for your custom one\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] training loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # --- Evaluation Phase ---\n",
        "    validation_loss = 0.0 # <-- ADDED: Initialize validation_loss\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs =  net(images)\n",
        "\n",
        "            # --- START OF ADDED/MODIFIED CODE ---\n",
        "            loss = criterion(outputs, labels) # <-- ADDED: Calculate loss on test data\n",
        "            validation_loss += loss.item() # <-- ADDED: Accumulate the validation loss\n",
        "            # --- END OF ADDED/MODIFIED CODE ---\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # --- Print Statistics and Step Scheduler ---\n",
        "    accuracy = 100 * correct // total\n",
        "    avg_val_loss = validation_loss / len(testloader) # <-- ADDED: Calculate average validation loss\n",
        "\n",
        "    print(f'Accuracy on the test set after epoch {epoch + 1}: {accuracy} %')\n",
        "    print(f'Validation loss after epoch {epoch + 1}: {avg_val_loss:.3f}') # <-- ADDED: Print validation loss\n",
        "\n",
        "    scheduler.step(avg_val_loss) # <-- MODIFIED: Pass validation loss to the scheduler\n",
        "    print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2lm01B0Omhi",
        "outputId": "debef613-2ec2-4dd0-a85f-db13af5c22aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set after epoch 1: 43 %\n",
            "Validation loss after epoch 1: 1.660\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 2: 56 %\n",
            "Validation loss after epoch 2: 1.198\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 3: 65 %\n",
            "Validation loss after epoch 3: 1.009\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 4: 70 %\n",
            "Validation loss after epoch 4: 0.834\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 5: 72 %\n",
            "Validation loss after epoch 5: 0.792\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 6: 69 %\n",
            "Validation loss after epoch 6: 0.893\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 7: 76 %\n",
            "Validation loss after epoch 7: 0.676\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 8: 78 %\n",
            "Validation loss after epoch 8: 0.643\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 9: 76 %\n",
            "Validation loss after epoch 9: 0.724\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 10: 77 %\n",
            "Validation loss after epoch 10: 0.654\n",
            "Current Learning Rate: 0.1\n",
            "Accuracy on the test set after epoch 11: 77 %\n",
            "Validation loss after epoch 11: 0.649\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 12: 85 %\n",
            "Validation loss after epoch 12: 0.429\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 13: 85 %\n",
            "Validation loss after epoch 13: 0.420\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 14: 86 %\n",
            "Validation loss after epoch 14: 0.411\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 15: 86 %\n",
            "Validation loss after epoch 15: 0.408\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 16: 86 %\n",
            "Validation loss after epoch 16: 0.406\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 17: 86 %\n",
            "Validation loss after epoch 17: 0.394\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 18: 86 %\n",
            "Validation loss after epoch 18: 0.393\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 19: 86 %\n",
            "Validation loss after epoch 19: 0.390\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 20: 86 %\n",
            "Validation loss after epoch 20: 0.385\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 21: 87 %\n",
            "Validation loss after epoch 21: 0.377\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 22: 87 %\n",
            "Validation loss after epoch 22: 0.384\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 23: 87 %\n",
            "Validation loss after epoch 23: 0.370\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 24: 87 %\n",
            "Validation loss after epoch 24: 0.370\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 25: 86 %\n",
            "Validation loss after epoch 25: 0.390\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 26: 87 %\n",
            "Validation loss after epoch 26: 0.381\n",
            "Current Learning Rate: 0.010000000000000002\n",
            "Accuracy on the test set after epoch 27: 87 %\n",
            "Validation loss after epoch 27: 0.376\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 28: 88 %\n",
            "Validation loss after epoch 28: 0.343\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 29: 88 %\n",
            "Validation loss after epoch 29: 0.341\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 30: 88 %\n",
            "Validation loss after epoch 30: 0.343\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 31: 88 %\n",
            "Validation loss after epoch 31: 0.341\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 32: 88 %\n",
            "Validation loss after epoch 32: 0.340\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 33: 88 %\n",
            "Validation loss after epoch 33: 0.338\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 34: 88 %\n",
            "Validation loss after epoch 34: 0.338\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 35: 88 %\n",
            "Validation loss after epoch 35: 0.339\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 36: 88 %\n",
            "Validation loss after epoch 36: 0.338\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 37: 88 %\n",
            "Validation loss after epoch 37: 0.337\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 38: 88 %\n",
            "Validation loss after epoch 38: 0.339\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 39: 88 %\n",
            "Validation loss after epoch 39: 0.338\n",
            "Current Learning Rate: 0.0010000000000000002\n",
            "Accuracy on the test set after epoch 40: 88 %\n",
            "Validation loss after epoch 40: 0.338\n",
            "Current Learning Rate: 0.00010000000000000003\n",
            "Accuracy on the test set after epoch 41: 88 %\n",
            "Validation loss after epoch 41: 0.339\n",
            "Current Learning Rate: 0.00010000000000000003\n",
            "Accuracy on the test set after epoch 42: 88 %\n",
            "Validation loss after epoch 42: 0.336\n",
            "Current Learning Rate: 0.00010000000000000003\n",
            "Accuracy on the test set after epoch 43: 88 %\n",
            "Validation loss after epoch 43: 0.336\n",
            "Current Learning Rate: 0.00010000000000000003\n",
            "Accuracy on the test set after epoch 44: 88 %\n",
            "Validation loss after epoch 44: 0.336\n",
            "Current Learning Rate: 0.00010000000000000003\n",
            "Accuracy on the test set after epoch 45: 88 %\n",
            "Validation loss after epoch 45: 0.337\n",
            "Current Learning Rate: 1.0000000000000004e-05\n",
            "Accuracy on the test set after epoch 46: 88 %\n",
            "Validation loss after epoch 46: 0.337\n",
            "Current Learning Rate: 1.0000000000000004e-05\n",
            "Accuracy on the test set after epoch 47: 88 %\n",
            "Validation loss after epoch 47: 0.336\n",
            "Current Learning Rate: 1.0000000000000004e-05\n",
            "Accuracy on the test set after epoch 48: 88 %\n",
            "Validation loss after epoch 48: 0.337\n",
            "Current Learning Rate: 1.0000000000000004e-06\n",
            "Accuracy on the test set after epoch 49: 88 %\n",
            "Validation loss after epoch 49: 0.338\n",
            "Current Learning Rate: 1.0000000000000004e-06\n",
            "Accuracy on the test set after epoch 50: 88 %\n",
            "Validation loss after epoch 50: 0.337\n",
            "Current Learning Rate: 1.0000000000000004e-06\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "\n",
        "\n",
        "1. We started with a simple 3 layer cnn using batch normalization layers, relu, and maxpooling layers\n",
        "2.   Then to prevent overfitting we introduced data augmentation using random rotations and crops and a learning rate scheduler to drop the learning every couple epochs to reach the mininmum faster\n",
        "3. Then we increased the width of the conv layers and added a dropout layer to decrease overfitting.\n",
        "4. Then we implemented residual blocks to add more layers without having to deal with vanishing gradients and replaced the max pooling with global average pooling to maintain more information.\n",
        "5. Finally we swapped out the optimizer from ADAM to SGD with momentum and switched the scheduler to a reduceLROnPlateau to adapt the learning rate more adaptively\n",
        "\n"
      ],
      "metadata": {
        "id": "xrIBQymo2nj1"
      }
    }
  ]
}